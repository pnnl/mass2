-*- text -*--------------------------------------------------
file: TODO
List of things to do to MASS2, time permitting
-------------------------------------------------------------
-------------------------------------------------------------
Battelle Memorial Institute
Pacific Northwest Laboratory
-------------------------------------------------------------
-------------------------------------------------------------
Created November 17, 1999 by William A. Perkins
Last Change: Tue Nov  1 11:41:20 2011 by William A. Perkins <d3g096@PE10900.pnl.gov>
-------------------------------------------------------------

$Id$

High Priority:
 
    * FLUX SOURCE/SINK in parallel

    * need to count transported scalars in scalar_source.dat not use
      the limit in the configuration file

    * check the id of a scalar source specification to make sure it
      has not already been specified

    * arbitrary id's in the scalar source specification, instead of
      consecutive integers

    * Open boundary condition for scalars that somehow understands
      flow reversal

    * utility to extract and integrate flows, velocities, temperature,
      and concentration along the border of a group of cells,
      e.g. the average concentration along the spillway at the
      downstream end of the grid. (partially done with mass2flux.pl)

    * Cartesian wind shear.

    * Supply a concentration with a volume source.

    * Limit the time frame for which output is produced; supply a list
      of output times

    * Come up with some signal files that will cause MASS2 to do
      things when they appear in the working directory. The existence
      of such files would be checked each time step at which time
      specific action is taken.  Actions might include

        * abort the run
        * dump a hotstart file (if enabled)
        * dump plot output  (if enabled)
        * combinations of the above

    * Transient CGNS that Visit understands

        * needs to be 2D
        * no CoordinateZ?
        * Need a BaseIterativeData node (but does not require time?)

Low Priority:

    * get parallel version working again; use Global Arrays 

    * utility to plot gage.nc directly -- perhaps some combination of
      perl script and gnuplot, maybe even with a GUI like pTk.

    * just do away with NetCDF gage output

    * add the capability to specify a non-point contaminant source
      that avoids the bed; this could be implemented by just using a
      different keyword (instead of BEDSOUCE) in scalar_source.dat

    * Open or radiant boundary condition.

    * Ability to identify when things aren't going well, e.g. really
      big negative depths, and just crash

Wetting/Drying Issues:

    * Check for "dry" cells when applying a scalar boundary condition.

    * Test scalars and wetting and drying, particularly TDG and
      temperature. 

    * check to see how dead zones affect mass source

Code Clean Up:

    * put the configuration files in CVS

    * configuration should use the CGNS macros from TE2THYS/PT6 

    * get rid of NetCDF; put gage output in text files?

    * check time series input to make sure it covers the entire
      simulation period

    * put the transient CGNS stuff back in (for visit)

    * change CGNS output variables to standard names (for visit)

    * start using doxygen for code documentation

    * find and apply an "indent"-like program for Fortran 90/95 --
      needs to do formatting and make sure variable names are all the
      same case

Wish List:

    * Graphical user interface

    * the transport test cases should be transport only for the most
      part

BUGS

    * when a scalar bed source is included in a cell that has a held
      concentration boundary, the source is cut in half.  This cropped
      up when ghost cells were added.  Until this is fixed, do not
      apply scalar bed sources to boundary cells.

    * Large block sizes can cause a silent SEGV on Linux systems
      because the stack is too small.  For example, the ap, ae, aw,
      an, as, bp, and source arrays in uvel_solve will not be usable.
      One has to increase the allowable stack size before running.
      It would probably be better to allocate them in each call.  

    * hydro_solve() and scalar_solve() declare several arrays that end
      up on the stack.  This causes a SEGV for large blocks when the
      shell stacksize limit is small.  These arrays should probably be
      allocated, but that might be slow.  

    * CGNS output in transport only mode cause a SEGV.

    * Visit doesn't like cell-centered CGNS output.  Why?
